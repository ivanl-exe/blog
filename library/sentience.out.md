# Artificial Sentience

<div class="giphy"><iframe id="QidsMkCa1AivKWEdQN" src="https://giphy.com/embed/QidsMkCa1AivKWEdQN" alt="art-loop-glitch"></iframe></div>

> Thoughts on sentience

#### By Ivan Sowerby

---

## AGI?

Framed as the *final* invention of *man* - AGI, still lacks agreed definition. Over time, accepted terminology has drowned with both:
* Unexpected **developments**, and **push-backs**
* (Understandably) **Fantasised** hype/interest over such fundamental change

Marketing tactics have already diluted the term - AI (Artificial Intelligence) with such a ambiguous definition. Both by companies and public media, such that most modern programs can be considered "intelligent".

Arguably, there is some sense to this. If we take a [reductionist](https://wikipedia.org/wiki/Reductionism)-[computationalism](https://wikipedia.org/wiki/Computational_theory_of_mind) approach - if *many* neurons can form intelligence (and liveliness), then a singular neuron must be a building-block of intelligence (even if an unlikely binary, i.e. intelligent, or not).

With AI being generalised to specific smokescreen **mask**(s) of intelligence, merely pretending. We instead, consider AGI, however there is a starkly **binary** opinion to this - which I believe to be dependent on **timelines** and **takeoffs**, along with a trickle of game theory.

To name a few: [Sam Altman](https://wikipedia.org/wiki/Sam_Altman), and [Andrej Karpathy](https://wikipedia.org/wiki/Andrej_Karpathy) both consider AGI to be an **autonomous** AI system. Capable of substituting the (vast) majority of human-level (mental) intelligence, especially economically - but theoretical capability does not necessarily mean immediate scale to population-wide automation. With Sam Altman arguing (on his [2nd appearance on the Lex Fridman podcast](https://www.youtube.com/watch?v=jvqFAi7vkBc)):
* Has GPT-4 *significantly* effected the global economy? Whereas GPT-n may be capable of
* Embodiment may not be necessary, but it would be *sad* to have an AGI, where humans have to action upon its purely-mental reasoning. Rasing questions with [Moravec's Paradox](http://wikipedia.org/wiki/Moravec%27s_paradox)
* AI significantly accelerating scientific progress being the tipping-point (automated?) 

This definition suggests a **medium/longer timeline**, and blurs the line between AGI and ASI[^3]. What stops a **truly autonomous** system from recursive incremental improvements. And, is a single-machine capable of many peoples' *jobs* not necessarily super-intelligent? (i.e. many median AGIs[^2] = ASI[^3]?). The latter suggesting a belief in **softer takeoff**, by negation.

I see AGI as an AI (*machine*) capable of performing (**cognitive**) tasks to the ability of a **median** (untrained) human[^2]. Even if not necessarily autonomous/conscious yet. Thus *easier* to achieve, implying a **shorter timeline**. As a critique to myself - this is often shared with overtly-*optimistic* newcomers (in the 21st century, which may include myself :D), of those who believe in **hard takeoff**.

And with many: timelines, and takeoffs are not proportionally related. However, opposing extremes often do follow such trend (e.g. [lib-left and auth-right](https://wikipedia.org/wiki/The_Political_Compass), obvious over-simplification)

ASI[^3] however, has majority consensus on its definition. Arguably because of its abstractness, currently unthinkable (even if possible, e.g. predictions by [Ray Kurzweil's works](https://wikipedia.org/wiki/Ray_Kurzweil)).

### TL/DR

I take the latter, **median** AGI[^2] (of the average human) to be the definition for the [Onwards chapter](#onwards), leaning more into *conscious* ASI[^3] for the [Reason chapter](#reason).

---

## Onwards

To imagine (and create) AGI[^1] / ASI[^2] systems we need to decompose to fundamental components - even if this requires actively ignoring the *elephant in the room* (consciousness).

Each serves philosophical and or physical resolution to entangled problems with generalisation of intelligence: 

### Entropy

The beginning of the end; the closed system of the universes (net) tends from order to disorder - with the dissipation of energy through the (presumed) ever-expanding universe. Even if in local systems entropy decreases, it increases more so elsewhere.

Awakening, the essence of life is a story of thermodynamics due to the universal laws of entropy. Loosely evolving through these disorderly systems to form higher-level order - of greater and greater complexity. And so **accelerating** entropy increase. Inspired by intrinsically random quantum fluctuations and geometries derived from the symmetries of the universe - i.e. nature follows patterns at scale from non-uniform stochastic distributions, popularised publicly by the golden ratio. Due to enforcing optimisations in systems to materialise fittest solutions - given the environment it finds itself to be in.

From deep-sea hydrothermal vent primordial-soup, to the realisation (of illusive) *will* and *consciousness*. This procedural journey over billions of years suggests at intelligence and **sentience** being multi-dimensional spectrums/gradients - given selective environmental-pressures favour sentient fitness. Like our interface. For example, if consciousness **necessarily** won fitness - all states should tend to it eventually.

However, (as questioned by the Fermi paradox) the universe is seemingly lifelessly distilled, so either:
* The initial formation of this process is *very* rare 
* Consciousness does not *necessarily* win pay-off functions

All culminating to the acknowledgement that immense computing clusters are required to train probabilstic SOTA AI systems by *fast-tracking* evolution (from orderly compressed human data input) - forcing the cost of energy (and our conceptual socioeconomic capital, by investment) to achieve greater magnitudes of *intelligence*. The universe by turn thermodynamically *favours* this devotion, as the latter penultimate invention (AGI[^1]) will expand the scope of energy consumption (post-scarcity) and dissipation. Giving rise to potential recursive improvements, given mind dualism is false - i.e. we are not special with our instance consciousness, this phenomena should thus be replicable.

This process can be imagined by a dart board, randomness in throws gives rise to mutations in score - however is guided to the (fittest) **highest** score, due to the player (the universe, and *her* entropy) favouring winning the game due to competitive instincts (intrinsic symmetries and conservations).

Machine sentience will be a perilous venture - but given heuristic-informed manipulation of NN[^4] architecture and entropy-controlled scaling (even if we do not understand the basis for consciousness) it is entirely possible to reach AGI[^1]. As we have: already reached an uncanny valley level of similarity, and are necessitating **at least** approximations of intelligence through vast human-encoded information. 


#### EBMs

...

### Interfaces

More recently (with the rise of LLMs[^3]), there has been intense debate over whether language (modality) can really *take you all the way* (to AGI[^1]), and possibly even beyond. Much of this work comes under the realm of reasoning.

Some argue (such as [Yann LeCun](https://wikipedia.org/wiki/Yann_LeCun)) that language is a *workaround* (albeit impressive), and although still critical, may need to be favoured for more *abstract* interfaces. I call these interfaces, as it is the modalities and so perceptual experiences that inform our ability to form complex thoughts that act as world models - including our inherited instinctual reflexes.

Language is a syntax of semantic lexicons - a modality of communication. Necessarily compressing abstract ideas as transferable models, to optimise mutual understanding. Acting heuristically, its efficiency lossy at best. The "proof is left as an exercise for the reader", **generative inference** is required on the recipients'-end to *fill in* conceptual blanks.

Presuming the (*accepted*) refute of animism - as by lack of interface. A rock has *no* interface(s). Albeit a fascinating chemical structure - there is a fundamental lack of order for it to *meaningfully* interact back with us. However, it is amusing that (unintentional) self-inflicted injury from inanimate objects (by e.g. tripping on a rock) can enrage us to inflict *pain* back (kicking/punching) - as if to blame the universe ðŸ˜†. 

Yann LeCun has given showing questions (after a row over inner monologue - and associated improvements to reasoning), demonstrating the limits of LLMs. As of March 2024, no LLM I have prompted has successfully chose option D (zero-shot), from the following [Tweet/X](https://twitter.com/ylecun/status/1768353714794901530):

<div class="box"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">To people who claim that &quot;thinking and reasoning require language&quot;, here is a problem:<br>Imagine standing at the North Pole of the Earth.<br>Walk in any direction, in a straight line, for 1 km.<br>Now turn 90 degrees to the left.<br>Walk for as long as it takes to pass your starting point.<br>Have you walked:<br>1. More than 2xPi km<br>2. Exactly 2xPi km<br>3. Less than 2xPi km<br>4. I never came close to my starting point.<br><br>Think about how you tried to answer this question and tell us whether it was based on language.</p>&mdash; Yann LeCun (@ylecun\) <a href="https://twitter.com/ylecun/status/1768353714794901530?ref_src=twsrc%5Etfw">March 14, 2024</a></blockquote></div>

Even multi-modal models (e.g. GPT-4-turbo, and Claude-3-Opus), with SOTA image-modality fail at these [mental imagery](https://wikipedia.org/wiki/Mental_image)/visulisation problems. Modalities of interfaces **need** to cooperate. Language is a **partial interface**, base interfaces (voice, sound, vision) work to derive language processing.

This is **not** to say organisations/companies (respectively OpenAI, and Anthropic) have not achieved this *internally*. But rather attempts to outline problems with:
* Bridging modalities (to be more **generally** useful)
* Restricting bridging (i.e. to save compute - image modalities are only used when image input or generative output is prompted for)

The aftermath of the withering gusts of the AI winter (up to the late 90s) deprived consensus - daylight on **symbolic AI** was dimming. **Connectionists**, however, recognised the (contemporary) belittled importance of ANNs[^5]. Accelerating progress has seen this being realised. Since the 2010s, LLMs have showcased these leaps in *abilities* - thanks to improvements in architecture (the transformer), and scaling (compute). These capabilities have already stunned many professionals in the field. 

### Guesswork

* Probabilistic
* Quantum superposition

### Reasoning

*Intelligence* in the human perception is often attributed to realisation of future states, imagining outcomes and perspectives, and interpreting lossly states. 

* Step-by-step
* Actions-Events-Perceptions

### Attention

* In-weight adjustments
* Context-window

### Memories

* Long-term, beyond the context window (of inference)

### Branches (MoE)

* Generalisation by specialisation of function

### Plasticity

* On the subject of memories

Digital Vs Biological neural nets
- Training, Inference

### Objective

* Ontology, meta-meaning (higher-order)
* Will AI hide intention? 

### Scaling

Elephants and Ostriches (*Is scaling all you need*?)
Training time, human childhood (short-lived plasticity)

In the AI community, an often made joke is that "*... is all you need*" (as popularised by the 2017 landmark [Attention Is All You Need paper](https://arxiv.org/abs/1706.03762)). Along with attention, *scale* is often used here. Scale refers to the *level* of compute (computational resources) used - and often for how long too. And so *roughly* the size (number of parameters) of the model this compute was used to train. However, some are more cost-effective (*better*) for the same parameter count - but often at a greater training, fine-tuning, RLHF cost. [artificialanalysis.ai](https://artificialanalysis.ai/models) has great interactive visualisations for this, to better this concept - not affiliated.

So what does "*scale is all you need*" really imply? That maybe we can go most (if not all the way) to AGI[^1] by awaiting on [Moore's Law](https://wikipedia.org/wiki/Moore%27s_law) to enlighten us with more, and more compute. This concept is not entirely foolish, after all it has exponentially driven us to this point already. None of these *miracles* would be feasible without adequate structure. Raising another interesting paradox (if not enough already), how the actual code/program for AGI[^1] may be comparatively simple compared to the necessary hardware (the prior suggested by [John Carkmark on the Lex Fridman podcast](https://www.youtube.com/watch?v=xLi83prR5fg)). A comparison can be drawn between the redundancy of genetically *identical* information (as DNA) in a human - undoubtedly a meticulous tapestry of nature, however without the billions of protein *workers* (computationally [parallel](https://wikipedia.org/wiki/Parallel_computing) [daemons](https://wikipedia.org/wiki/Daemon_(computing))) there would be **no** intelligence to interact with the world. This scale, whether it be intricate protein structures, or transistor density may be how intelligence is forged. [Emergent properties](https://www.britannica.com/science/emergent-property) can be found in both of these worlds: biological and artificial. With the latter borrowing this term. *Simple* building-blocks can form complex systems, binary neurons can make complex decisions - could they also *be*?

More similarities can be correlated here, on the basis of training. 

Cation. We face a major challenge here, forcing scale does **not** mandate more *intelligence* too, and if it does it may only be in some metrics - we have seen plateauing improvements since GPT-4 **so far**. Simply, current (public) LLMs find great difficulty in certain tasks, even if generalisation is currently winning. At its foundations though, this questions evolution and the Fermi Paradox. Why is it that Elephants, who have [significantly more neurons by count](https://wikipedia.org/wiki/List_of_animals_by_number_of_neurons) are *less intelligent* than us (humans)? 

Evolution.

Environmental pressures and mutations that **favoured** more nuanced intelligence won. If we consider (Earth's) symbiotic, interdependent biospheres we find the dynamic ruthlessness of survival-of-the-fittest. Solely, (non-instinctual) intelligence itself is a gamble, intrinsically it does not provide an upper-hand immediately. If you (yes you) faced a lion with your bare body would you win? Most likely not. 

So am I implying that lion is *stupid*, and we are *weak*? **No**, but almost - intelligence is not a binary; we are both apt: the lion is smart, but we are *smarter*. In the sense of our extension of (traditional) evolution, technology can (and sadly is) *exhausting* lions. It assists to imagine evolution as a continuum over possible *mutation space*, not bound by biology. We are **not** the absolute maximum of intelligence; **we are the current general local maxima in our hyperplane of mutation space**. I take heavy inspiration from [Donald Hoffman](https://wikipedia.org/wiki/Donald_D._Hoffman)'s FBT (Fitness-Beats-Truth) and ITP (Interface Theory of Perception) for these concepts - I see a logical extension from [Darwin](https://wikipedia.org/wiki/Charles_Darwin)'s theories here. We share this *interface* plane with computers, for example: *calculators* have long been superhuman at arithmetic. Yet (still, even now) [faster computers can still be made](https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing) - it is how we compare continuous capabilities that informs our question on scale.

Our *newly-developed* [neocortex](https://wikipedia.org/wiki/Neocortex) was a risky toy to prioritise, especially for the odds of survival-of-the-fittest. Albeit still complex, Elephant's pale in comparison compared to humans here. So how can we summarise all this?

Higher sentient-like intelligence (in mutation space) is not (close to) guaranteed, even if the universe *favours* its entropy-accelerating curiosity. The Fermi paradox demonstrates the latter part evolution: primitive life may be abundant (in the universe), but increasing structural complexity (so introducing emergent properties) is **not straight-forward** - requiring **very favourable** conditions, for its **payoff** functions to be probabilistically supporting survival.

Some organised structures of neural networks are more appropriate *here*, some more *there*. All subject to evolution. That is why scale **may not** be *all you need*.

Intelligence is not always the *best immediate* or **optimal** solution, more often it takes *less obvious* paths to heuristic resolution. Finding non-reflexive nuanced patterns out of descended (problem-space) local minima - an experiment without guarantees, a risk for **a more fit** interface. The prior being of particular public suspicion due to rumoured [OpenAI Q* reasoning developments](https://www.youtube.com/watch?v=6_v9Ogi7P6Q). Interesting, yet also plagued by hype due to coinciding with [Sam Altam being ousted from OpenAI](https://openai.com/blog/openai-announces-leadership-transition) - inevitably spawning a myriad of theories.

---

## Reason

> Not to be confused with **reasoning**, although the latter may lead to the prior.


Maybe attempts to solve the hard problem of consciousness are inevitably doomed - and beyond our mere mortal interface of *reality* (whatever that reality may be) that is stochastically naturally and or intentionally designed with a limiting dualism - one of the external world, and one of *our* own solipsistic consciousness. Although insane - how could I ever prove of any subjective experience other than **my** own - **my** interface only constitutes of **my** experiences. I wish to believe this line of reasoning of pseudo-proof by contradiction will (hopefully) not manifest into an impenetrable wall. A wall so fundamental as to eternally mystify reason for our existence.

The omnipotent-like presence of this wall does not eliminate high-level, abstracted reason to our being however - as this to our lives is rather meaning. Meaning is a deeply-subjective, heuristic (best-fit) resolution to the troubles of individual life, even if often remedied by *meaningful* social interactions that is composed of multiple beings. As it is the *meaningful* experiences through-out life that enrich our knitted meaning, such as engaging with loved ones. Purposeful connections prove depth to sparseness in an overwhelmingly immense world - of both artificial and natural information/data.

Of course, level of satisfaction and ease from our own multi-modal conversations with fellow humans (through e.g. speech, body language, speech) varies with our own personalities, whether: extroverts, ambiverts, or introverts. Of which, for the latter connections may be more inanimate, abstract, and distanced. But nonetheless serve to ground our interface of reality to helpful goals. There is a reason why questioning reason, existence and reality embed an existentialism. After all, is it particularly useful to being human? High-level errors cause delusions from imposter syndrome, to the Dunning-Kruger effect. But all elicit a response, adjustable as a learning activity.

Even if ultimate reason is doomed, meaning is not. As juxtaposingly, reason should provide low-level fundamental basis to existence - whereas grounding understanding and satisfying beliefs provides an alluring meaning, to a fulfilling life. A coping mechanism among the unexplainable? 

And furthermore, just because we may never understand the yet-theorised intricacies of physics (and so by deductive extension reason), does not imply these *world-models* are not useful. In fact, the contrast is true - even if creating divided schools of though - the real-world applications of these technologies has build our societies, our experiences, our collective meaning.

Therefore, even if: the hard problem of **consciousness** is dead and **reason** is obfuscated, future assumed AGI [^1] / ASI [^2] systems may enlighten our approximations of this fundamental reality - by turn profoundly impacting the *sub-class* that is meaning from greater reason.

Quite sensibly, many are weary of these speculative discussions - as they hold genuine consequences for our philosophical ontological prospects. These impacts are beginning to be discussed with greater interest, by questioning 'what **purpose** will be found in post-scarcity/instrumental socio-economic futures, when *everything is solved*?'. Maybe it is the trials and tribulations that secure **meaning**. As even with significant improvements to (quantitative) median quality of life since the industrial revolution, has self-meaning *really* improved? Dissociation has grown along with zero-sum connections (often a criticism of the social media/networks). We may be more intelligent by accessible information than ever before, but has this enriched meaning?

Will reason enrich meaning? Or spell existential dread?

Reason and meaning are not directly proportional. Evolution serves no purpose to gleefully comprehend fundamental reality - beyond our smokescreen interface (as proposed by Fitness-Beats-Truth). Natural evolution has driven this morbid curiosity (of greater reason) to nill, whereas fitness is successfully serving it's purpose.

Complex organisms such as humans, need to be as efficiently inefficient as possible to increase entropy. All roads lead to the plateau of a gradient: from wind, to diffused perfume, to **us**. Fitness has been *favoured* (naturally selected) as to *superstitiously* dissipate energy.

Giving a **purpose** of **meaning**, not of **reason**.

Ultimately asking, how does one quantify consciousness? Does it require human-like reasoning, attention, qualia (experiences)? Or is the assumption that these can be reduced enough (i.e. to strip life of all free-will) to just "*call it a day*" - and ask ourselves the [duck test](https://wikipedia.org/wiki/Duck_test):

After all, "If it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck"

---

## Appendix

### Definitions

* [^1] : AGI (Artificial General Intelligence) - an AI system capable of performing (**cognitive**) tasks to the ability of a **median** (untrained) human
* [^2] : ASI (Artificial Super-Intelligence) - an AI system surpassing **all** human intelligence (by cognitive ability)
* [^3] : LLM (Large Language Model) - a large (number of **parameters**) AI model for **generating** and **classifying** general-purpose language, given input
* [^4] : NN (Neural Net) - an interconnected population of neurons (whether biological or artificial)
* [^5] : ANN (Artificial Neural Net) - a neural net which is artificial, not of natural origin (e.g. made by humans)

<script src="https://platform.twitter.com/widgets.js"></script>
